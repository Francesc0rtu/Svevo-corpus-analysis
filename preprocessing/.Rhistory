if(min(X[,index])+0.1 < max(X[,index])-0.1) {
for( j in seq(min(X[,index])+0.1, max(X[,index])-0.1,0.1)) {
left <- X[which(X[,index] <= j),,drop=FALSE]
right <- X[which(X[,index] > j),,drop=FALSE]
# compute left and right errors
err_left <- (nrow(left) - length(which(left[,3] == names(which.max(table(left))))))/nrow(left)
err_right <- (nrow(right) - length(which(right[,3] == names(which.max(table(right))))))/nrow(right)
err <- err_left + err_right
if(err < minim) {
minim = err
i = index
t = j
}
}
}
}
return(c(i,t))
}
build_decision_tree <- function(X,y) {
if (should_stop(y)) {
# establish which is the value of the node
y_sign <- names(which.max(table(y)))
# return the correct label to print out the tree
return(paste0(species[y_sign]))
}
else {
l <- best_branch(X,y)
i <- l[1]
t <- l[2]
#build left and right subtrees
d_tree$AddChild(build_decision_tree(X[which(X[,i] < t),,drop=FALSE],X[which(X[,i] < t),,drop=FALSE][,3]))
d_tree$AddChild(build_decision_tree(X[which(X[,i] > t),,drop=FALSE],X[which(X[,i] > t),,drop=FALSE][,3]))
return(paste0(lab[i], " < ", t, " ", sample(1:100, 1)))
}
}
build_decision_tree(X,y)
build_decision_tree <- function(X,y) {
if (should_stop(y)) {
# establish which is the value of the node
y_sign <- names(which.max(table(y)))
# return the correct label to print out the tree
return("leaf")
}
else {
l <- best_branch(X,y)
i <- l[1]
t <- l[2]
#build left and right subtrees
d_tree$AddChild(build_decision_tree(X[which(X[,i] < t),,drop=FALSE],X[which(X[,i] < t),,drop=FALSE][,3]))
d_tree$AddChild(build_decision_tree(X[which(X[,i] > t),,drop=FALSE],X[which(X[,i] > t),,drop=FALSE][,3]))
return(paste0(lab[i], " < ", t, " ", sample(1:100, 1)))
}
}
build_decision_tree(X,y)
plot(iris$Petal.Length, iris$Petal.Width, xlab="Petal length", ylab="Petal width", col=c(4,2,3)[unclass(iris$Species)], pch=16)
### PARAMETERS
# input: X (n x p) number of observations * number of independent variables
n = nrow(iris)
p = 2 # petal width and petal length
# let's say we want to select just iris setosa,
# X and y are the input of our algorithm
lab = c("length", "width")
species <- levels(iris$Species)
X <- cbind(iris$Petal.Length, iris$Petal.Width, iris$Species)
y <- iris$Species
library(data.tree)
d_tree <- Node$new("Decision tree")
kmin = 2
should_stop <- function(y) {
if (length(which(y != names(which.max(table(y))))) < kmin) {
return(TRUE)
}
else if(nrow(table(y)) == 1){
return(TRUE)
}
else {
return(FALSE)
}
}
best_branch <- function(X,y) {
minim = 100
i = 1
t = 1
# evaluate which is the best splitting axis
# and the best splitting value
for (index in c(1,2)) {
if(min(X[,index])+0.1 < max(X[,index])-0.1) {
for( j in seq(min(X[,index])+0.1, max(X[,index])-0.1,0.1)) {
left <- X[which(X[,index] <= j),,drop=FALSE]
right <- X[which(X[,index] > j),,drop=FALSE]
# compute left and right errors
err_left <- (nrow(left) - length(which(left[,3] == names(which.max(table(left))))))/nrow(left)
err_right <- (nrow(right) - length(which(right[,3] == names(which.max(table(right))))))/nrow(right)
err <- err_left + err_right
if(err < minim) {
minim = err
i = index
t = j
}
}
}
}
return(c(i,t))
}
build_decision_tree <- function(X,y) {
if (should_stop(y)) {
# establish which is the value of the node
y_sign <- names(which.max(table(y)))
# return the correct label to print out the tree
return("leaf")
}
else {
l <- best_branch(X,y)
i <- l[1]
t <- l[2]
#build left and right subtrees
d_tree$AddChild(build_decision_tree(X[which(X[,i] < t),,drop=FALSE],X[which(X[,i] < t),,drop=FALSE][,3]))
d_tree$AddChild(build_decision_tree(X[which(X[,i] > t),,drop=FALSE],X[which(X[,i] > t),,drop=FALSE][,3]))
return(paste0(lab[i], " < ", t, " ", sample(1:100, 1)))
}
}
build_decision_tree(X,y)
d_tree
build_decision_tree <- function(X,y) {
if (should_stop(y)) {
# establish which is the value of the node
y_sign <- names(which.max(table(y)))
message("HERE LEAF")
# return the correct label to print out the tree
return("leaf")
}
else {
l <- best_branch(X,y)
i <- l[1]
t <- l[2]
#build left and right subtrees
d_tree$AddChild(build_decision_tree(X[which(X[,i] < t),,drop=FALSE],X[which(X[,i] < t),,drop=FALSE][,3]))
d_tree$AddChild(build_decision_tree(X[which(X[,i] > t),,drop=FALSE],X[which(X[,i] > t),,drop=FALSE][,3]))
return(paste0(lab[i], " < ", t, " ", sample(1:100, 1)))
}
}
build_decision_tree(X,y)
d_tree
d_tree <- Node$new("Decision tree")
kmin = 2
should_stop <- function(y) {
if (length(which(y != names(which.max(table(y))))) < kmin) {
return(TRUE)
}
else if(nrow(table(y)) == 1){
return(TRUE)
}
else {
return(FALSE)
}
}
best_branch <- function(X,y) {
minim = 100
i = 1
t = 1
# evaluate which is the best splitting axis
# and the best splitting value
for (index in c(1,2)) {
if(min(X[,index])+0.1 < max(X[,index])-0.1) {
for( j in seq(min(X[,index])+0.1, max(X[,index])-0.1,0.1)) {
left <- X[which(X[,index] <= j),,drop=FALSE]
right <- X[which(X[,index] > j),,drop=FALSE]
# compute left and right errors
err_left <- (nrow(left) - length(which(left[,3] == names(which.max(table(left))))))/nrow(left)
err_right <- (nrow(right) - length(which(right[,3] == names(which.max(table(right))))))/nrow(right)
err <- err_left + err_right
if(err < minim) {
minim = err
i = index
t = j
}
}
}
}
return(c(i,t))
}
build_decision_tree <- function(X,y) {
if (should_stop(y)) {
# establish which is the value of the node
y_sign <- names(which.max(table(y)))
message("HERE LEAF")
# return the correct label to print out the tree
return("leaf")
}
else {
l <- best_branch(X,y)
i <- l[1]
t <- l[2]
#build left and right subtrees
d_tree$AddChild(build_decision_tree(X[which(X[,i] < t),,drop=FALSE],X[which(X[,i] < t),,drop=FALSE][,3]))
d_tree$AddChild(build_decision_tree(X[which(X[,i] > t),,drop=FALSE],X[which(X[,i] > t),,drop=FALSE][,3]))
return(paste0(lab[i], " < ", t, " ", sample(1:100, 1)))
}
}
build_decision_tree(X,y)
d_tree
plot(d_tree)
print(d_tree)
# read corpus
corpus <- read.csv("../csv/svevo_letters.csv", sep=";", encoding = "UTF-8")
setwd("~/DSSC/Svevo-corpus-analysis/preprocessing")
# read corpus
corpus <- read.csv("../csv/svevo_letters.csv", sep=";", encoding = "UTF-8")
# clean corpus and save csv
corpus <- clean_text(corpus)
library(tm)
library(dplyr)
library(tidytext)
library(textstem)
library(data.table)
# the following function performs all the steps needed to preprocess the whole corpus
#adjust columns, lower text, remove punctuation and numbers, then remove stop-words
clean_text <- function(corpus) {
corpus$n <- corpus$n + 1
names(corpus)[names(corpus) == 'n'] <- 'letter_number'
names(corpus)[names(corpus) == 'corpus'] <- 'pair'
corpus$pair <- gsub("Schmitz","Svevo", corpus$pair)
corpus$tokens <- NA
sw_list_ita <- scan("stopwords_ita.txt", what="", sep="\n")
sw_list_eng <- scan("stopwords_eng.txt", what="", sep="\n")
for(i in 1:nrow(corpus)) {
#remove punctuation
corpus$tokens[i] <- gsub("[^a-zA-Z]+"," ", tolower(corpus$text[i]))
#remove words shorter than 3 chars
corpus$tokens[i] <- gsub('\\b\\w{1,2}\\b',"", tolower(corpus$tokens[i]))
#remove stop words
if (grepl("ENG",corpus$mainLanguage[i])) {
lang = "english"
corpus$tokens[i] <- removeWords(corpus$tokens[i], c(stopwords(lang),  sw_list_eng))
}
if (grepl("ITA",corpus$mainLanguage[i])) {
lang = "italian"
corpus$tokens[i] <- removeWords(corpus$tokens[i], c(stopwords(lang), sw_list_ita))
}
if (grepl("FRE",corpus$mainLanguage[i])) {
lang = "french"
corpus$tokens[i] <- removeWords(corpus$tokens[i], stopwords(lang))
}
if (grepl("GER",corpus$mainLanguage[i])) {
lang = "german"
corpus$tokens[i] <- removeWords(corpus$tokens[i], stopwords(lang))
}
}
return(corpus)
}
# read corpus
corpus <- read.csv("../csv/svevo_letters.csv", sep=";", encoding = "UTF-8")
# clean corpus and save csv
corpus <- clean_text(corpus)
corpus$tokens[90]
corpus <- system('python lemmatize.py corpus')
help("system")
corpus <- system2('python lemmatize.py corpus')
attr(corpus, "status")
corpus <- system2('python lemmatize.py corpus', stdout=TRUE, stderr=TRUE)
corpus <- system2('python lemmatize.py', stdout=TRUE, stderr=TRUE)
corpus <- system2('python lemmatize.py', stdout=TRUE, stderr=TRUE)
setwd("~/DSSC/Svevo-corpus-analysis/preprocessing")
corpus <- system2('python lemmatize.py', stdout=TRUE, stderr=TRUE)
corpus <- system2('python lemmatize.py')
corpus <- system2('python lemmatize.py', stdout=TRUE, stderr=TRUE)
python lemmatize.py
corpus <- system2('python lemmatize.py lemmatize corpus', stdout=TRUE, stderr=TRUE)
corpus <- system('python lemmatize.py lemmatize corpus', stdout=TRUE, stderr=TRUE)
corpus <- system('python lemmatize.py lemmatize corpus')
# read corpus
corpus <- read.csv("../csv/svevo_letters.csv", sep=";", encoding = "UTF-8")
# clean corpus and save csv
corpus <- clean_text(corpus)
corpus <- system('python lemmatize.py corpus')
corpus <- system('python lemmatize.py corpus')
help("system")
corpus <- system('python lemmatize.py', corpus)
# read corpus
corpus <- read.csv("../csv/svevo_letters.csv", sep=";", encoding = "UTF-8")
# clean corpus and save csv
corpus <- clean_text(corpus)
# call python script to lemmatize tokens wrt the language used in each letter
system('python lemmatize.py')
# call python script to lemmatize tokens wrt the language used in each letter
system('python lemmatize.py')
library(tm)
C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
ap_lda <- LDA(tdm, k = 5, method = "Gibbs", control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
# Load dataset with lemmatized words
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
# see to who are the letters sent to
receivers <- corpus %>%
group_by(pair) %>%
count(pair, sort = TRUE)  %>%
filter(n > 5) %>%
ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) +
geom_bar(stat = "identity") +
labs(x = "receiver", y = "number of letters") +
theme(legend.title = element_blank())
receivers
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# plot words for the whole corpus
corpus_words %>%
filter(n > 250) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
library(dplyr)
library(tidyverse)
library(patchwork)
# all the words contained in the letters and the m=number of time they appear
subset_Livia <- corpus %>%
filter(pair == "Svevo Livia") %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
subset_Joyce <- corpus %>%
filter(pair == "Svevo Joyce") %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# plot words for the whole corpus
livia <- subset_Livia %>%
mutate(word = reorder(word, n)) %>%
slice_max(order_by = n, n = 10) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
joyce <- subset_Joyce %>%
mutate(word = reorder(word, n)) %>%
slice_max(order_by = n, n = 10) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
livia + joyce
document_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(letter_number, word)
document_words$total_num <- rep(document_words %>% summarize(total = sum(n)), nrow(document_words))
# compute tf-idf for each word in the corpus
document_tf_idf <- document_words %>%
bind_tf_idf(word, letter_number, n)
head(document_tf_idf)
# plot words for document 1
document_tf_idf %>%
filter(letter_number == 1 & n >1) %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(tf_idf, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
#select first 10 words for each document wrt tf-idf
document_tf_idf %>%
group_by(letter_number) %>%
slice_max(order_by = tf_idf, n =10)
# for document 1 check words and look at how they are classified (positive or negative)
ap_sentiments <- document_tf_idf %>%
filter(letter_number == 1) %>%
inner_join(get_sentiments("bing"), by = c(word = "word"))
names(ap_sentiments)[names(ap_sentiments) == 'n'] <- 'count'
ap_sentiments %>%
count(sentiment, word, wt = count) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ylab("Contribution to sentiment")
library(tm)
C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
ap_lda <- LDA(tdm, k = 5, method = "Gibbs", control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
install.packages('topicmodels')
library(tm)
C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
ap_lda <- LDA(tdm, k = 5, method = "Gibbs", control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
library(tm)
C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
ap_lda <- LDA(tdm, k = 5, method = "Gibbs", control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
# Load dataset with lemmatized words
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
# see to who are the letters sent to
receivers <- corpus %>%
group_by(pair) %>%
count(pair, sort = TRUE)  %>%
filter(n > 5) %>%
ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) +
geom_bar(stat = "identity") +
labs(x = "receiver", y = "number of letters") +
theme(legend.title = element_blank())
receivers
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# plot words for the whole corpus
corpus_words %>%
filter(n > 250) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
library(dplyr)
library(tidyverse)
library(patchwork)
# all the words contained in the letters and the m=number of time they appear
subset_Livia <- corpus %>%
filter(pair == "Svevo Livia") %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
subset_Joyce <- corpus %>%
filter(pair == "Svevo Joyce") %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# plot words for the whole corpus
livia <- subset_Livia %>%
mutate(word = reorder(word, n)) %>%
slice_max(order_by = n, n = 10) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
joyce <- subset_Joyce %>%
mutate(word = reorder(word, n)) %>%
slice_max(order_by = n, n = 10) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
livia + joyce
document_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(letter_number, word)
document_words$total_num <- rep(document_words %>% summarize(total = sum(n)), nrow(document_words))
# compute tf-idf for each word in the corpus
document_tf_idf <- document_words %>%
bind_tf_idf(word, letter_number, n)
head(document_tf_idf)
# plot words for document 1
document_tf_idf %>%
filter(letter_number == 1 & n >1) %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(tf_idf, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
#select first 10 words for each document wrt tf-idf
document_tf_idf %>%
group_by(letter_number) %>%
slice_max(order_by = tf_idf, n =10)
# for document 1 check words and look at how they are classified (positive or negative)
ap_sentiments <- document_tf_idf %>%
filter(letter_number == 1) %>%
inner_join(get_sentiments("bing"), by = c(word = "word"))
names(ap_sentiments)[names(ap_sentiments) == 'n'] <- 'count'
ap_sentiments %>%
count(sentiment, word, wt = count) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ylab("Contribution to sentiment")
library(tm)
C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
ap_lda <- LDA(tdm, k = 5, method = "Gibbs", control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
ap_lda <- LDA(tdm, k = 5, control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
C <- Corpus(VectorSource(corpus$tokens))
library(tidyr)
ap_lda <- LDA(tdm, k = 5, control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
library(tm)
View(tdm)
View(tdm)
library(tm)
C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
ap_lda <- LDA(tdm, k = 5, control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
# per-topic-per-word probabilities
ap_topics <- tidy(ap_lda, matrix = "beta")
library(topicmodels)
C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))
ap_lda <- LDA(tdm, k = 5, control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.
