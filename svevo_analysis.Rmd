---
title: "ML_final_project"
output: html_document
---


Load the provided corpus of documents and make a first analysis of words.

```{r}
# Load dataset with lemmatized words


corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones

```


Carry out a first basic analysis of the dataset:


```{r}
library(ggplot2)
library(dplyr)

# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) + 
  geom_bar( ) +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position="none") +
  labs(x = "language", y = "number of letters")

# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)


# see to who are the letters sent to
receivers <- corpus %>%
  group_by(pair) %>%
  count(pair, sort = TRUE)  %>%
  filter(n > 5) %>%
  ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) + 
  geom_bar(stat = "identity") + 
  labs(x = "receiver", y = "number of letters") +
  theme(legend.title = element_blank()) 


receivers

```

Analyze words wrt the entire corpus. Look at which are the most used terms in the entire corpus.
```{r}
library(dplyr)
library(tidyverse)
library(tidytext)


# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
  unnest_tokens(word, tokens) %>%
  count(word, sort = TRUE)


# plot words for the whole corpus
corpus_words %>%
  filter(n > 250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL)



```

Re-do previous analysis but wrt to who the letters are addressed to.
In particular, 10 most important words in letters to Livia and Joyce are compared.

```{r}
library(dplyr)
library(tidyverse)
library(patchwork)

# all the words contained in the letters and the m=number of time they appear
subset_Livia <- corpus %>%
  filter(pair == "Svevo Livia") %>%
  unnest_tokens(word, tokens) %>%
  count(word, sort = TRUE)

subset_Joyce <- corpus %>%
  filter(pair == "Svevo Joyce") %>%
  unnest_tokens(word, tokens) %>%
  count(word, sort = TRUE)

# plot words for the whole corpus
livia <- subset_Livia %>%
  mutate(word = reorder(word, n)) %>%
  slice_max(order_by = n, n = 10) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL) 

joyce <- subset_Joyce %>%
  mutate(word = reorder(word, n)) %>%
  slice_max(order_by = n, n = 10) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL) 


livia + joyce

```

Analysis of most important words wrt the single document.
Build tf-idf term-document matrix:
```{r}
document_words <- corpus %>%
  unnest_tokens(word, tokens) %>%
  count(letter_number, word)

document_words$total_num <- rep(document_words %>% summarize(total = sum(n)), nrow(document_words))

# compute tf-idf for each word in the corpus
document_tf_idf <- document_words %>%
  bind_tf_idf(word, letter_number, n)

head(document_tf_idf)

# plot words for document 1
document_tf_idf %>%
  filter(letter_number == 1 & n >1) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL)

```

Find 10 most relevant words in each document using tf-idf. 
Build a table accounting for most relevant words in each document.

```{r}

#select first 10 words for each document wrt tf-idf
document_tf_idf %>%
  group_by(letter_number) %>%
  slice_max(order_by = tf_idf, n =10)


# for document 1 check words and look at how they are classified (positive or negative)
ap_sentiments <- document_tf_idf %>%
  filter(letter_number == 1) %>%
  inner_join(get_sentiments("bing"), by = c(word = "word"))


names(ap_sentiments)[names(ap_sentiments) == 'n'] <- 'count'

ap_sentiments %>%
  count(sentiment, word, wt = count) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

```


# LDA 

First of all build term-document matrix

```{r}
library(tm)
library(topicmodels)

C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))


ap_lda <- LDA(tdm, k = 5, control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.


# per-topic-per-word probabilities
ap_topics <- tidy(ap_lda, matrix = "beta")

# find first 10 terms most related with each topic
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# per-document-per-topic probabilities
# for each topic-document pair assign a value
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents

```

Measure perplexity in our trained LDA.
```{r}

library(topicmodels)
library(doParallel)
library(ggplot2)
library(scales)

burnin = 1000
iter = 1000
keep = 50
# define our "full data"
full_data  <- tdm
n <- nrow(full_data)

#-----------validation--------
k <- 5 # number of topics
splitter <- sample(1:n, round(n * 0.75))
train_set <- full_data[splitter, ]
valid_set <- full_data[-splitter, ]
fitted <- LDA(train_set, k = k, method = "Gibbs",
                          control = list(burnin = burnin, iter = iter, keep = keep) )
perplexity(fitted, newdata = train_set)
perplexity(fitted, newdata = valid_set)

```

Using perplexity and CV to choose k:

```{r, eval = FALSE}
#----------------5-fold cross-validation, different numbers of topics----------------

cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
   library(topicmodels)
})

folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- seq(2,50,2) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))

system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
   k <- candidate_k[j]
   results_1k <- matrix(0, nrow = folds, ncol = 2)
   colnames(results_1k) <- c("k", "perplexity")
   for(i in 1:folds){
      train_set <- full_data[splitfolds != i , ]
      valid_set <- full_data[splitfolds == i, ]
      
      fitted <- LDA(train_set, k = k, method = "Gibbs",
                    control = list(burnin = burnin, iter = iter, keep = keep))
      results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
   }
   return(results_1k)
}
})
stopCluster(cluster)
results_df <- as.data.frame(results)

ggplot(results_df, aes(x = k, y = perplexity)) +
   geom_point() +
   geom_smooth(se = FALSE) +
   ggtitle("5-fold cross-validation of topic modelling",
           "(The points represent five different models fit for each candidate number of topics)") +
   labs(x = "K", y = "Perplexity when fitting the trained model to the hold-out set")


```

See topics for k = 20.

```{r, eval = FALSE}

fitted <- LDA(tdm, k = 20, method = "Gibbs",
                    control = list(burnin = burnin, iter = iter, keep = keep) )

# per-topic-per-word probabilities
ap_topics <- tidy(fitted, matrix = "beta")

# per-topic-per-word probabilities
ap_topics <- tidy(fitted, matrix = "beta")

# find first 10 terms most related with each topic
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```


# Main Topics analysis

An analysis of suitable value for the number of topics k is provided in `LDA.R`.
We choose an intermediate value for k, which ensures a good coherence between topics
while keeping perplexity low.

Now, we proceed with the analysis of our corpus, selecting just italian letters (826 documents) 
and analyzing the main topics found by setting k = 26.

Load italian letters.
```{r}
# Load dataset with lemmatized words

corpus_ita <- read.csv("csv/cleaned_svevo_dataset_ITA.csv", sep=",", encoding = "UTF-8")
corpus_ita$tokens <- corpus_ita$lemmatized_tokens # make tokens the lemmatized ones

```

Build LDA model, with k = 26.
```{r}
library(textmineR)

#compute document-term-matrix
dtm <- CreateDtm(doc_vec = corpus_ita$lemmatized_tokens, # character vector of documents
                 doc_names = corpus_ita$letter_number, # document names
                 ngram_window = c(1, 1), # minimum and maximum n-gram length
                 lower = FALSE, 
                 remove_punctuation = FALSE, 
                 stopword_vec = c(),
                 remove_numbers = FALSE, 
                 verbose = TRUE,
                 cpus = 4) # default is all available cpus on the system


#random fit
set.seed(12345)
num_topics <- 26
#compute LDA with fixing value of K
lda_ita <- FitLdaModel(dtm = dtm, 
                     k = num_topics,
                     iterations = 800, #  recommend at least 500 iterations or more
                     burnin = 180,
                     alpha = 0.1,
                     beta = 0.05,
                     optimize_alpha = TRUE,
                     calc_likelihood = TRUE,
                     calc_coherence = TRUE,
                     calc_r2 = TRUE,
                     cpus = 4) 

saveRDS(lda_ita, file = "LDA/LDA_model_ita.rds")
```

Analysis of main topics obtained.
```{r}
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts. 
lda_ita$prevalence <- colSums(lda_ita$theta) / sum(lda_ita$theta) * 100

lda_ita$top_terms <- GetTopTerms(phi = lda_ita$phi, M = 10)

# textmineR has a naive topic labeling tool based on probable bigrams
lda_ita$labels <- LabelTopics(assignments = lda_ita$theta > 0.05, 
                            dtm = dtm,
                            M = 1)

head(lda_ita$labels)

# put them together, with coherence into a summary table
lda_ita$summary <- data.frame(topic = rownames(lda_ita$phi),
                            label = lda_ita$labels,
                            coherence = round(lda_ita$coherence, 3),
                            prevalence = round(lda_ita$prevalence,3),
                            top_terms = apply(lda_ita$top_terms, 2, function(x){
                              paste(x, collapse = ", ")
                            }),
                            stringsAsFactors = FALSE)

#print summary table
lda_ita$summary[ order(lda_ita$summary$prevalence, decreasing = TRUE) , ]#[ 1:20 , ]

SummarizeTopics(lda_ita)

```

We can see some topics are overlapping, so let's choose a subset of the most distant ones.


```{r}
lda_ita$topic_linguistic_dist <- CalcHellingerDist(lda_ita$phi)
lda_ita$hclust <- hclust(as.dist(lda_ita$topic_linguistic_dist), "ward.D2")
lda_ita$hclust$labels <- paste(lda_ita$hclust$labels, lda_ita$labels[ , 1])

plot(lda_ita$hclust)

```

```{r}
library(Matrix)
# see distances in a table
d <- round(lda_ita$topic_linguistic_dist,3)

table(d)

# we need to find the first n topics which are the most distant from all the others
rowSums(d)

most_distant <- names(sort(rowSums(d), decreasing = TRUE)[1:10])
most_relevant <- (lda_ita$summary[ order(lda_ita$summary$prevalence, decreasing = TRUE) , ][ 1:10, ])$topic

chosen_topics <- intersect(most_distant, most_relevant) 

lda_ita$summary[ lda_ita$summary$topic %in% chosen_topics , ]

```



```{r}
library(textmineR)

#random fit
set.seed(12345)
num_topics <- 5
#compute LDA with fixing value of K
lda_ita2 <- FitLdaModel(dtm = dtm, 
                     k = num_topics,
                     iterations = 2000, #  recommend at least 500 iterations or more
                     burnin = 180,
                     alpha = 0.1,
                     beta = 0.05,
                     optimize_alpha = TRUE,
                     calc_likelihood = TRUE,
                     calc_coherence = TRUE,
                     calc_r2 = TRUE,
                     cpus = 4) 

lda_ita2$prevalence <- colSums(lda_ita2$theta) / sum(lda_ita2$theta) * 100

lda_ita2$top_terms <- GetTopTerms(phi = lda_ita2$phi, M = 10)

# textmineR has a naive topic labeling tool based on probable bigrams
lda_ita2$labels <- LabelTopics(assignments = lda_ita2$theta > 0.05, 
                            dtm = dtm,
                            M = 1)

head(lda_ita2$labels)

# put them together, with coherence into a summary table
lda_ita2$summary <- data.frame(topic = rownames(lda_ita2$phi),
                            label = lda_ita2$labels,
                            coherence = round(lda_ita2$coherence, 3),
                            prevalence = round(lda_ita2$prevalence,3),
                            top_terms = apply(lda_ita2$top_terms, 2, function(x){
                              paste(x, collapse = ", ")
                            }),
                            stringsAsFactors = FALSE)

lda_ita2$summary[ order(lda_ita2$summary$prevalence, decreasing = TRUE) , ]



```


Analyze how topics evolved over time:

```{r}
#input: one topic
#output: its trend over time

# obtain document-topic probabilities
#lda_ita2$theta
# number are those of italian letters in the original corpus

topic_time <- data.frame(lda_ita2$theta)
topic_time$date <- format(as.Date(corpus_ita$date, format="%d/%m/%Y"),"%Y")

head(topic_time)
#remove NA
topic_time<-na.omit(topic_time)

```

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)


sum_topic <- topic_time %>%
  group_by(date) %>%
  summarise(across(everything(), sum))

sum_topic  %>%
    gather(key,value, t_1, t_2, t_3, t_4, t_5) %>%
    ggplot() +
    geom_line(aes(x=date, y=value, colour=key, group = 1),ylab="")+
    geom_point(aes(x=date, y=value, colour=key, group = 1),ylab="")

```



We could try to select just those documents which present a more clear discussion about just one or a few topics.

# ```{r}
# library(dplyr)
# library(textmineR)
# 
# # add reference to letter number
# topic_time$letter_number <- rownames(topic_time)
# 
# mark_doc <- topic_time %>% 
#   filter(if_any(starts_with('t_'), ~ . > 0.6))
# 
# corpus_450 <- corpus_ita[which(corpus_ita$letter_number %in% mark_doc$letter_number),]
# 
# 
# #compute document-term-matrix
# dtm_450 <- CreateDtm(doc_vec = corpus_450$lemmatized_tokens, # character vector of documents
#                  doc_names = corpus_450$letter_number, # document names
#                  ngram_window = c(1, 1), # minimum and maximum n-gram length
#                  lower = FALSE, 
#                  remove_punctuation = FALSE, 
#                  stopword_vec = c(),
#                  remove_numbers = FALSE, 
#                  verbose = TRUE,
#                  cpus = 4) # default is all available cpus on the system
# 
# 
# 
# #random fit
# set.seed(12345)
# num_topics <- 5
# #compute LDA with fixing value of K
# lda_450 <- FitLdaModel(dtm = dtm_450, 
#                      k = num_topics,
#                      iterations = 2000, #  recommend at least 500 iterations or more
#                      burnin = 180,
#                      alpha = 0.1,
#                      beta = 0.05,
#                      optimize_alpha = TRUE,
#                      calc_likelihood = TRUE,
#                      calc_coherence = TRUE,
#                      calc_r2 = TRUE,
#                      cpus = 4) 
# 
# # Analysis of main topics obtained.
# 
# lda_450$prevalence <- colSums(lda_450$theta) / sum(lda_450$theta) * 100
# 
# lda_450$top_terms <- GetTopTerms(phi = lda_450$phi, M = 10)
# 
# # textmineR has a naive topic labeling tool based on probable bigrams
# lda_450$labels <- LabelTopics(assignments = lda_450$theta > 0.05, 
#                             dtm = dtm_450,
#                             M = 1)
# 
# head(lda_450$labels)
# 
# # put them together, with coherence into a summary table
# lda_450$summary <- data.frame(topic = rownames(lda_450$phi),
#                             label = lda_450$labels,
#                             coherence = round(lda_450$coherence, 3),
#                             prevalence = round(lda_450$prevalence,3),
#                             top_terms = apply(lda_450$top_terms, 2, function(x){
#                               paste(x, collapse = ", ")
#                             }),
#                             stringsAsFactors = FALSE)
# 
# #print summary table
# lda_450$summary[ order(lda_450$summary$prevalence, decreasing = TRUE) , ]#[ 1:20 , ]
# 
# SummarizeTopics(lda_450)
#```