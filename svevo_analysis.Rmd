---
title: "ML_final_project"
output: html_document
---


Load the provided corpus of documents and make a first analysis of words.

```{r}
# Load dataset with lemmatized words


corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones

```


Carry out a first basic analysis of the dataset:


```{r}
library(ggplot2)
library(dplyr)

# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) + 
  geom_bar( ) +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position="none") +
  labs(x = "language", y = "number of letters")

# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)


# see to who are the letters sent to
receivers <- corpus %>%
  group_by(pair) %>%
  count(pair, sort = TRUE)  %>%
  filter(n > 5) %>%
  ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) + 
  geom_bar(stat = "identity") + 
  labs(x = "receiver", y = "number of letters") +
  theme(legend.title = element_blank()) 


receivers

```

Analyze words wrt the entire corpus. Look at which are the most used terms in the entire corpus.
```{r}
library(dplyr)
library(tidyverse)
library(tidytext)


# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
  unnest_tokens(word, tokens) %>%
  count(word, sort = TRUE)


# plot words for the whole corpus
corpus_words %>%
  filter(n > 250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL)



```

Re-do previous analysis but wrt to who the letters are addressed to.
In particular, 10 most important words in letters to Livia and Joyce are compared.

```{r}
library(dplyr)
library(tidyverse)
library(patchwork)

# all the words contained in the letters and the m=number of time they appear
subset_Livia <- corpus %>%
  filter(pair == "Svevo Livia") %>%
  unnest_tokens(word, tokens) %>%
  count(word, sort = TRUE)

subset_Joyce <- corpus %>%
  filter(pair == "Svevo Joyce") %>%
  unnest_tokens(word, tokens) %>%
  count(word, sort = TRUE)

# plot words for the whole corpus
livia <- subset_Livia %>%
  mutate(word = reorder(word, n)) %>%
  slice_max(order_by = n, n = 10) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL) 

joyce <- subset_Joyce %>%
  mutate(word = reorder(word, n)) %>%
  slice_max(order_by = n, n = 10) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL) 


livia + joyce

```

Analysis of most important words wrt the single document.
Build tf-idf term-document matrix:
```{r}
document_words <- corpus %>%
  unnest_tokens(word, tokens) %>%
  count(letter_number, word)

document_words$total_num <- rep(document_words %>% summarize(total = sum(n)), nrow(document_words))

# compute tf-idf for each word in the corpus
document_tf_idf <- document_words %>%
  bind_tf_idf(word, letter_number, n)

head(document_tf_idf)

# plot words for document 1
document_tf_idf %>%
  filter(letter_number == 1 & n >1) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL)

```

Find 10 most relevant words in each document using tf-idf. 
Build a table accounting for most relevant words in each document.

```{r}

#select first 10 words for each document wrt tf-idf
document_tf_idf %>%
  group_by(letter_number) %>%
  slice_max(order_by = tf_idf, n =10)


# for document 1 check words and look at how they are classified (positive or negative)
ap_sentiments <- document_tf_idf %>%
  filter(letter_number == 1) %>%
  inner_join(get_sentiments("bing"), by = c(word = "word"))


names(ap_sentiments)[names(ap_sentiments) == 'n'] <- 'count'

ap_sentiments %>%
  count(sentiment, word, wt = count) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

```


# LDA 

First of all build term-document matrix

```{r}
library(tm)
library(topicmodels)

C <- Corpus(VectorSource(corpus$tokens))
tdm <- DocumentTermMatrix(C, control = list(bounds = list(global = c(5, Inf))))


ap_lda <- LDA(tdm, k = 5, control = list(seed = 1234)) # A LDA_VEM topic model with 5 topics.


# per-topic-per-word probabilities
ap_topics <- tidy(ap_lda, matrix = "beta")

# find first 10 terms most related with each topic
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# per-document-per-topic probabilities
# for each topic-document pair assign a value
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents

```

Measure perplexity in our trained LDA.
```{r}

library(topicmodels)
library(doParallel)
library(ggplot2)
library(scales)

burnin = 1000
iter = 1000
keep = 50
# define our "full data"
full_data  <- tdm
n <- nrow(full_data)

#-----------validation--------
k <- 5 # number of topics
splitter <- sample(1:n, round(n * 0.75))
train_set <- full_data[splitter, ]
valid_set <- full_data[-splitter, ]
fitted <- LDA(train_set, k = k, method = "Gibbs",
                          control = list(burnin = burnin, iter = iter, keep = keep) )
perplexity(fitted, newdata = train_set)
perplexity(fitted, newdata = valid_set)

```

Using perplexity and CV to choose k:

```{r, eval = FALSE}
#----------------5-fold cross-validation, different numbers of topics----------------

cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)
clusterEvalQ(cluster, {
   library(topicmodels)
})

folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- seq(2,50,2) # candidates for how many topics
clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))

system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
   k <- candidate_k[j]
   results_1k <- matrix(0, nrow = folds, ncol = 2)
   colnames(results_1k) <- c("k", "perplexity")
   for(i in 1:folds){
      train_set <- full_data[splitfolds != i , ]
      valid_set <- full_data[splitfolds == i, ]
      
      fitted <- LDA(train_set, k = k, method = "Gibbs",
                    control = list(burnin = burnin, iter = iter, keep = keep))
      results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
   }
   return(results_1k)
}
})
stopCluster(cluster)
results_df <- as.data.frame(results)

ggplot(results_df, aes(x = k, y = perplexity)) +
   geom_point() +
   geom_smooth(se = FALSE) +
   ggtitle("5-fold cross-validation of topic modelling",
           "(The points represent five different models fit for each candidate number of topics)") +
   labs(x = "K", y = "Perplexity when fitting the trained model to the hold-out set")


```

See topics for k = 20.

```{r}

fitted <- LDA(tdm, k = 20, method = "Gibbs",
                    control = list(burnin = burnin, iter = iter, keep = keep) )

# per-topic-per-word probabilities
ap_topics <- tidy(fitted, matrix = "beta")

# per-topic-per-word probabilities
ap_topics <- tidy(fitted, matrix = "beta")

# find first 10 terms most related with each topic
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
