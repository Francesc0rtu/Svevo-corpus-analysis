---
title: "ML_final_project"
output: html_document
---

Load the provided corpus of documents and make a first analysis of words.

```{r}
library(tm)
library(dplyr)
library(tidytext)
library(textstem)



corpus <- read.csv("svevo_letters.csv", sep=";", encoding = "UTF-8")

corpus$tokens <- NA

# lower text, remove punctuation and numbers, then remove stop-words
clean_text <- function(corpus) {
  for(i in 1:nrow(corpus)) {
    #remove punctuation 
    corpus$tokens[i] <- gsub("[^[:alpha:][:space:]]*","", tolower(corpus$text[i]))
    
    #remove stop words
    if (grepl("ENG",corpus$languages[i]))
      corpus$tokens[i] <- removeWords(corpus$tokens[i], stopwords("english"))
    if (grepl("ITA",corpus$languages[i]))
      corpus$tokens[i] <- removeWords(corpus$tokens[i], stopwords("italian"))
    if (grepl("FRE",corpus$languages[i]))
      corpus$tokens[i] <- removeWords(corpus$tokens[i], stopwords("french"))
    
    # lemmatization 
    corpus$tokens[i] <- lemmatize_words(corpus$tokens[i])
  }
  
  return(corpus)
} 

corpus <- clean_text(corpus)

# analysis of words in cleaned corpus
letter_words <- corpus %>%
  unnest_tokens(word, tokens) %>%
  count(index, word, sort = TRUE)


total_words <- letter_words %>% 
  group_by(index) %>% 
  summarize(total = sum(n))

corpus_words <- left_join(letter_words, total_words)

corpus_words

# compute tf-idf for each word in the corpus
letters_tf_idf <- corpus_words %>%
  bind_tf_idf(word, index, n)



```

Try to do some LDA analysis of topics:

```{r lda}

library(topicmodels)

#create corpus object from the dataframe
Corpus_obj <- Corpus(VectorSource(corpus$token))

#Compute Document term matrix from the corpus object
TDM <- DocumentTermMatrix(Corpus_obj, control = list(bounds = list(global = c(5, Inf))))

#Compute the LDA
topicModel <- LDA(TDM, 5, method = "VEM")

#Print the first 10 words for each topics
terms(topicModel, 17)

#
```
