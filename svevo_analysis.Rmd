---
title: "ML_final_project"
output: html_document
---

Load the provided corpus of documents and make a first analysis of words.

```{r}
library(tm)
library(dplyr)
library(tidytext)
library(textstem)
library(data.table)


corpus <- read.csv("svevo_letters.csv", sep=";", encoding = "UTF-8")

corpus$n <- corpus$n + 1
names(corpus)[names(corpus) == 'n'] <- 'letter_number'
corpus$tokens <- NA

# add one column accounting for the most used language in each letter
corpus$first_language <- substr(corpus$languages, 1,3)

sw_list_ita <- scan("stopwords_ita.txt", what="", sep="\n")
sw_list_eng <- scan("stopwords_eng.txt", what="", sep="\n")

# lower text, remove punctuation and numbers, then remove stop-words
clean_text <- function(corpus) {
  for(i in 1:nrow(corpus)) {
    #remove punctuation 
    corpus$tokens[i] <- gsub("[^a-zA-Z]+"," ", tolower(corpus$text[i]))
    #remove words shorter than 3 chars
    corpus$tokens[i] <- gsub('\\b\\w{1,2}\\b',"", tolower(corpus$tokens[i]))
    
    #remove stop words
    if (grepl("ENG",corpus$first_language[i])) {
      lang = "english"
      corpus$tokens[i] <- removeWords(corpus$tokens[i], c(stopwords(lang),  sw_list_eng))
    }
    if (grepl("ITA",corpus$first_language[i])) {
      lang = "italian"
      corpus$tokens[i] <- removeWords(corpus$tokens[i], c(stopwords(lang), sw_list_ita))
    }
    if (grepl("FRE",corpus$first_language[i])) {
      lang = "french"
      corpus$tokens[i] <- removeWords(corpus$tokens[i], stopwords(lang))
    }
    if (grepl("GER",corpus$first_language[i])) {
      lang = "german"
      corpus$tokens[i] <- removeWords(corpus$tokens[i], stopwords(lang))
    }

    
    # lemmatization, for the moment I couldn't find a lemmatizer which works for every language
    # corpus$tokens[i] <- lemmatize_words(corpus$tokens[i], lang = corpus$first_language[i])
    
    # stemming
    stemDocument(corpus$text[i],language = lang)
  }
  
  return(corpus)
} 

corpus <- clean_text(corpus)
fwrite(corpus, paste0("cleaned_svevo_dataset.csv"),append=TRUE, col.names = TRUE)

```

Carry out a first basic analysis of the dataset:


```{r}
library(ggplot2)

# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(first_language), fill=as.factor(first_language))) + 
  geom_bar( ) +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position="none") +
  labs(x = "language", y = "number of letters")

# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
```
Analyze words wrt the entire corpus.
```{r}
library(dplyr)
library(tidyverse)

# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
  unnest_tokens(word, tokens) %>%
  count(word, sort = TRUE)


# plot words for the whole corpus
corpus_words %>%
  filter(n > 250) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL)

```
Analysis of most important words wrt the single document.
Build tf-idf term-document matrix:
```{r}
document_words <- corpus %>%
  unnest_tokens(word, tokens) %>%
  count(letter_number, word)

document_words$total_num <- rep(document_words %>% summarize(total = sum(n)), nrow(document_words))

# compute tf-idf for each word in the corpus
document_tf_idf <- document_words %>%
  bind_tf_idf(word, letter_number, n)

head(document_tf_idf)

# plot words for document 1
document_tf_idf %>%
  filter(letter_number == 1 & n >1) %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(fill = "#597199")+
  labs(y = NULL)

```

Find 15 most relevant words in each document using tf-idf.

```{r}

#select first 10 words for each document wrt tf-idf
document_tf_idf %>%
  group_by(letter_number) %>%
  slice_max(order_by = tf_idf, n =10)


# for document 1 check words and look at how they are classified (positive or negative)
ap_sentiments <- document_tf_idf %>%
  filter(letter_number == 1) %>%
  inner_join(get_sentiments("bing"), by = c(word = "word"))


names(ap_sentiments)[names(ap_sentiments) == 'n'] <- 'count'

ap_sentiments %>%
  count(sentiment, word, wt = count) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

```
