geom_point(data = df1, aes(x = year, y = string), group = 1)
ggplot() +
geom_col(data = lang1, aes(x = l, y = c, fill = l), group = 1) +
labs(x = "Languages", y = "Occurrences") +
geom_text(show.legend = FALSE) +
theme(legend.position = "none")
df <- read.csv("svevo_letters.csv", sep = ";", encoding = "UTF-8")
setwd("C:/Users/berna/Desktop/Sberneun/UniversitÃ /Magistrale/ANNO 1/Introduction to machine learning/Progetto")
df <- read.csv("svevo_letters.csv", sep = ";", encoding = "UTF-8")
plot(df$languages)
summary(df)
lang <- data.frame(0, 0, 0, 0, 0)
names(lang) <- c("ITA", "ENG", "FRE", "GER", "UND" )
lang$ITA[1]
for (x in df$languages) {
if(grepl("ITA", x)){
lang$ITA[1] <- lang$ITA[1] + 1
}
if(grepl("GER", x)){
lang$GER[1] <- lang$GER[1] + 1
}
if(grepl("ENG", x)){
lang$ENG[1] <- lang$ENG[1] + 1
}
if(grepl("FRE", x)){
lang$FRE[1] <- lang$FRE[1] + 1
}
if(grepl("UND", x)){
lang$UND[1] <- lang$UND[1] + 1
}
}
plot(lang)
lang
barplot(lang)
langMat <- as.matrix(lang)
langMat
barplot(langMat)
langMat[1,]
l <- c("ITA", "ENG", "FRE", "GER", "UND" )
c <- langMat[1,]
lang1 <- data.frame(l,c)
lang1
library(ggplot2)
ggplot() +
geom_col(data = lang1, aes(x = l, y = c, fill = l), group = 1) +
labs(x = "Languages", y = "Occurrences") +
geom_text(show.legend = FALSE) +
theme(legend.position = "none")
ggplot() +
geom_col(data = lang1, aes(x = l, y = c, fill = l), group = 1) +
labs(x = "Languages", y = "Occurrences") +
geom_text(show.legend = FALSE) +
theme(legend.position = "none")
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
setwd("~/GitHub/Svevo-corpus-analysis")
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
Carry out a first basic analysis of the dataset:
```{r}
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
# see to who are the letters sent to
receivers <- corpus %>%
group_by(pair) %>%
count(pair, sort = TRUE)  %>%
filter(n > 5) %>%
ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) +
geom_bar(stat = "identity") +
labs(x = "receiver", y = "number of letters") +
theme(legend.title = element_blank())
receivers
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
# see to who are the letters sent to
receivers <- corpus %>%
group_by(pair) %>%
count(pair, sort = TRUE)  %>%
filter(n > 5) %>%
ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) +
geom_bar(stat = "identity") +
labs(x = "receiver", y = "number of letters") +
theme(legend.title = element_blank())
receivers
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# plot words for the whole corpus
corpus_words %>%
filter(n > 250) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
# see to who are the letters sent to
receivers <- corpus %>%
group_by(pair) %>%
count(pair, sort = TRUE)  %>%
filter(n > 5) %>%
ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) +
geom_bar(stat = "identity") +
labs(x = "receiver", y = "number of letters") +
theme(legend.title = element_blank())
receivers
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# Load dataset with lemmatized words
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
# see to who are the letters sent to
receivers <- corpus %>%
group_by(pair) %>%
count(pair, sort = TRUE)  %>%
filter(n > 5) %>%
ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) +
geom_bar(stat = "identity") +
labs(x = "receiver", y = "number of letters") +
theme(legend.title = element_blank())
receivers
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# Load dataset with lemmatized words
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
library(ggplot2)
# count the number of letters for each language
ggplot(corpus, aes(x=as.factor(mainLanguage), fill=as.factor(mainLanguage))) +
geom_bar( ) +
scale_fill_brewer(palette = "Set1") +
theme(legend.position="none") +
labs(x = "language", y = "number of letters")
# italian letters are the: 92.39%
count(corpus[which(corpus$first_language == "ITA"),])/nrow(corpus)
# see to who are the letters sent to
receivers <- corpus %>%
group_by(pair) %>%
count(pair, sort = TRUE)  %>%
filter(n > 5) %>%
ggplot(aes(x = substr(pair,7,20), y = n, fill=as.factor(pair))) +
geom_bar(stat = "identity") +
labs(x = "receiver", y = "number of letters") +
theme(legend.title = element_blank())
receivers
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
library(dplyr)
library(tidyverse)
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
library(dplyr)
library(tidyverse)
library(patchwork)
# all the words contained in the letters and the m=number of time they appear
subset_Livia <- corpus %>%
filter(pair == "Svevo Livia") %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
library(dplyr)
library(tidyverse)
library(patchwork)
# all the words contained in the letters and the m=number of time they appear
subset_Livia <- corpus %>%
filter(pair == "Svevo Livia") %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
library(topicmodels)
library(tm)
library(textmineR)
##################  FUNCTIONS ###################
multiple_K_coherence <- function(max_K, dtm){
set.seed(12345)
coher <- c(1:max_K)
for(i in c(1:max_K)){
model <- FitLdaModel(dtm = dtm,
k = i,
iterations = 500, #  recommend at least 500 iterations or more
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = FALSE,
cpus = 4)
coher[i] <- mean(model$coherence)
}
return(coher)
}
corpus <- data.frame(read.csv("csv/cleaned_svevo_dataset.csv"))
#compute document-term-matrix
dtm <- CreateDtm(doc_vec = corpus$lemmatized_tokens, # character vector of documents
doc_names = corpus$letter_number, # document names
ngram_window = c(1, 1), # minimum and maximum n-gram length
lower = FALSE, # lowercase - this is the default value
remove_punctuation = FALSE, # punctuation - this is the default
stopword_vec = c(),
remove_numbers = FALSE, # numbers - this is the default
verbose = TRUE,
cpus = 4) # default is all available cpus on the system
### Choose the best number of topics based on the number of topics
max_K <- 20 #Max number of topics we want
coer_on_multiple_K <- multiple_K_coherence(max_K, dtm) # takes a lot of time!!!!!!!!
plot(c(1:max_K), coer_on_multiple_K, type='l')  #plot results
coer_on_multiple_K
plot(c(1:max_K), coer_on_multiple_K, type='l')  #plot results
coer_on_multiple_K
############### ONE MODEL ANALYSIS ###################################
#random fit
set.seed(12345)
num_topics <- 6 # MUST TAKES THE BEST OF THE COMPUTATION ABOVE
#compute LDA with fixing value of K
model <- FitLdaModel(dtm = dtm,
k = num_topics,
iterations = 800, #  recommend at least 500 iterations or more
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 4)
#print log-likelihood (higher is better)----TO DECIDE NUMBER OF ITERATIONS---not so important for us
plot(model$log_likelihood, type = "l")
#print summory of topic-coherence
summary(model$coherence)
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
# prevalence should be proportional to alpha
plot(model$prevalence, model$alpha, xlab = "prevalence", ylab = "alpha")
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
# textmineR has a naive topic labeling tool based on probable bigrams
model$labels <- LabelTopics(assignments = model$theta > 0.05,
dtm = dtm,
M = 1)
head(model$labels)
# put them together, with coherence into a summary table
model$summary <- data.frame(topic = rownames(model$phi),
label = model$labels,
coherence = round(model$coherence, 3),
prevalence = round(model$prevalence,3),
top_terms = apply(model$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
#print summary table
model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ][ 1:10 , ]
#compute LDA with fixing value of K
model <- FitLdaModel(dtm = dtm,
k = num_topics,
iterations = 800, #  recommend at least 500 iterations or more
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 4)
#print log-likelihood (higher is better)----TO DECIDE NUMBER OF ITERATIONS---not so important for us
plot(model$log_likelihood, type = "l")
#print summory of topic-coherence
summary(model$coherence)
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
# prevalence should be proportional to alpha
plot(model$prevalence, model$alpha, xlab = "prevalence", ylab = "alpha")
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
# prevalence should be proportional to alpha
plot(model$prevalence, model$alpha, xlab = "prevalence", ylab = "alpha")
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
# textmineR has a naive topic labeling tool based on probable bigrams
model$labels <- LabelTopics(assignments = model$theta > 0.05,
dtm = dtm,
M = 1)
head(model$labels)
# put them together, with coherence into a summary table
model$summary <- data.frame(topic = rownames(model$phi),
label = model$labels,
coherence = round(model$coherence, 3),
prevalence = round(model$prevalence,3),
top_terms = apply(model$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
#print summary table
model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ][ 1:10 , ]
library(tidytext)
corpus <- read.csv("csv/cleaned_svevo_dataset.csv", sep=",", encoding = "UTF-8")
corpus
corpus$tokens <- corpus$lemmatized_tokens # make tokens the lemmatized ones
corpus$tokens
sentiments %>% slice(sample(1:nrow(sentiments)))
sentiments %>% slice(sample(1:nrow(sentiments)))
summary(corpus)
corpus$text[mainLanguage == ENG]
corpus$text[which(mainLanguage == "ENG")]
corpus$text[which(corpus$mainLanguage == "ENG")]
sentiments$sentiment[which(sentimens$word) = corpus$text[which(corpus$mainLanguage == "ENG")]]
sentiments$sentiment[which(sentimens$word) == corpus$text[which(corpus$mainLanguage == "ENG")]]
sentiments$sentiment[which(sentiments$word) == corpus$text[which(corpus$mainLanguage == "ENG")]]
sentiments$sentiment[which((sentiments$word) == corpus$text[which(corpus$mainLanguage == "ENG"))]]
sent <-
?slice
?slice
?sentiments
corpus$tokens[7]
print[x]
corpus$tokens[7]
corpus$tokens[which(corpus$mainLanguage == "ENG")]
for (x in corpus$tockens[7]) {
print[x]
}
print(0)
for (x in corpus$tockens[7]) {
print[x]
print(0)
}
corpus$tokens[7]
wordcloud(corpus$tokens, scale = c(2, 1), min.freq = 50, colors = rainbow(30))
install.packages("wordcloud")
library(worvdcloud)
wordcloud(corpus$tokens, scale = c(2, 1), min.freq = 50, colors = rainbow(30))
library(wordcloud)
wordcloud(corpus$tokens, scale = c(2, 1), min.freq = 50, colors = rainbow(30))
wordcloud(corpus$tokens, scale = c(2, 1), min.freq = 50, colors = rainbow(30))
wordcloud(corpus$tokens, scale = c(2, 1), min.freq = 100, colors = rainbow(30))
wordcloud(corpus$tokens, scale = c(12, 1), min.freq = 100, colors = rainbow(30))
wordcloud(corpus$tokens, scale = c(12, 4), min.freq = 100, colors = rainbow(30))
wordcloud(corpus$tokens, scale = c(2, 1), min.freq = 100, colors = rainbow(30))
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
wordcloud(corpus$tokens, scale = c(2, 1), min.freq = 100, colors = rainbow(30))
# all the words contained in the letters and the m=number of time they appear
corpus_words <- corpus %>%
unnest_tokens(word, tokens) %>%
count(word, sort = TRUE)
# plot words for the whole corpus
corpus_words %>%
filter(n > 250) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col(fill = "#597199")+
labs(y = NULL)
plot(c(1:max_K), coer_on_multiple_K, type='l')  #plot results
coer_on_multiple_K <- multiple_K_coherence(max_K, dtm) # takes a lot of time!!!!!!!!
plot(c(1:max_K), coer_on_multiple_K, type='l')  #plot results
sentiments
sentiments %>% slice(sample(1:nrow(sentiments)))
write.csv(coer_on_multiple_K, "coherhence.csv")
coer_on_multiple_K
ggplot() +
geom_point(data = coer_on_multiple_K, aes(x = c(1:max_K), y = coer_on_multiple_K))
ggplot() +
geom_point(data = fortify(coer_on_multiple_K), aes(x = c(1:max_K), y = coer_on_multiple_K))
ggplot() +
geom_point(aes(x = c(1:max_K), y = coer_on_multiple_K))
ggplot() +
geom_point(aes(x = c(1:max_K), y = coer_on_multiple_K)) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K))
ggplot() +
geom_point(aes(x = 6, y = coer_on_multiple_K)) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K))
ggplot() +
geom_point(aes(x = 6, y = coer_on_multiple_K[6])) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K))
ggplot() +
geom_point(aes(x = 5, y = coer_on_multiple_K[5])) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K))
ggplot() +
geom_point(aes(x = 5, y = coer_on_multiple_K[5]), col = "red") +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K)) +
plot(c(1:max_K), coer_on_multiple_K, type='l')  #plot results
ggplot() +
geom_point(aes(x = 5, y = coer_on_multiple_K[5]), col = "red") +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K)) +
xlab("K") +
ylab("Coherence")
ggplot() +
geom_point(aes(x = 5, y = coer_on_multiple_K[5]), col = "red", size = 3) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K)) +
xlab("K") +
ylab("Coherence")
ggplot() +
geom_point(aes(x = 5, y = coer_on_multiple_K[5]), col = "red", size = 3) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K), col = "green") +
xlab("K") +
ylab("Coherence")
ggplot() +
geom_point(aes(x = 5, y = coer_on_multiple_K[5]), col = "red", size = 3) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K), col = "blu") +
xlab("K") +
ylab("Coherence")
ggplot() +
geom_point(aes(x = 5, y = coer_on_multiple_K[5]), col = "red", size = 3) +
geom_line(aes(x = c(1:max_K), y = coer_on_multiple_K), col = "violet") +
xlab("K") +
ylab("Coherence")
